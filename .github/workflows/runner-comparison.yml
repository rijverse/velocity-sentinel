name: Runner Performance Comparison

on:
  workflow_dispatch:
    inputs:
      runner_type:
        description: 'Runner type to test'
        required: true
        default: 'ubuntu-latest'
        type: choice
        options:
          - 'ubuntu-latest'
          - 'ubuntu-latest-4-cores'
          - 'ubuntu-latest-8-cores'
          - 'ubuntu-latest-16-cores'
          - 'windows-latest'
          - 'macos-latest'
          - 'self-hosted'
      test_iterations:
        description: 'Number of test iterations'
        required: false
        default: '3'
        type: string

jobs:
  runner-benchmark:
    runs-on: ${{ github.event.inputs.runner_type }}
    
    strategy:
      matrix:
        node-version: [18.x, 20.x]
    
    steps:
    - name: Runner Information
      run: |
        set -e
        echo "üèÉ‚Äç‚ôÇÔ∏è Testing Runner: ${{ github.event.inputs.runner_type }}"
        echo "üíª Runner OS: ${{ runner.os }}"
        echo "üèóÔ∏è Runner Architecture: ${{ runner.arch }}"
        echo "üîß Node.js Version: ${{ matrix.node-version }}"
        echo "üîÑ Test Iterations: ${{ github.event.inputs.test_iterations }}"
        
        # System information
        echo "## üñ•Ô∏è System Information" >> $GITHUB_STEP_SUMMARY
        echo "- **Runner Type**: ${{ github.event.inputs.runner_type }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Operating System**: ${{ runner.os }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Architecture**: ${{ runner.arch }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Node.js Version**: ${{ matrix.node-version }}" >> $GITHUB_STEP_SUMMARY
        
        # Hardware information (Linux only)
        if [ "${{ runner.os }}" = "Linux" ]; then
          echo "- **CPU Info**: $(nproc) cores" >> $GITHUB_STEP_SUMMARY
          echo "- **Memory**: $(free -h | awk '/^Mem:/ {print $2}')" >> $GITHUB_STEP_SUMMARY
          echo "- **Disk Space**: $(df -h / | awk 'NR==2 {print $2}')" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **CPU Info**: Not available on this OS" >> $GITHUB_STEP_SUMMARY
          echo "- **Memory**: Not available on this OS" >> $GITHUB_STEP_SUMMARY
          echo "- **Disk Space**: Not available on this OS" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "RUNNER_TYPE=${{ github.event.inputs.runner_type }}" >> $GITHUB_ENV
        echo "TEST_ITERATIONS=${{ github.event.inputs.test_iterations }}" >> $GITHUB_ENV
        echo "WORKFLOW_START_TIME=$(date +%s)" >> $GITHUB_ENV
    
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'yarn'
    
    - name: Dependency Installation Benchmark
      run: |
        set -e
        echo "üì¶ Starting dependency installation benchmark..."
        INSTALL_START=$(date +%s)
        
        # Clean yarn cache to ensure clean install
        yarn cache clean
        
        # Install dependencies with timing
        time yarn install --frozen-lockfile --verbose > install.log 2>&1
        
        INSTALL_END=$(date +%s)
        INSTALL_DURATION=$((INSTALL_END - INSTALL_START))
        if ! [[ "$INSTALL_DURATION" =~ ^[0-9]+$ ]]; then
          echo "Error: INSTALL_DURATION is not a number!" >&2
          exit 1
        fi
        echo "INSTALL_DURATION=${INSTALL_DURATION}" >> $GITHUB_ENV
        
        echo "‚úÖ Dependencies installed in ${INSTALL_DURATION}s"
        
        # Package count and size information (Linux only)
        if [ "${{ runner.os }}" = "Linux" ]; then
          PACKAGE_COUNT=$(yarn list --depth=0 2>/dev/null | grep -c "‚îÄ " || echo "0")
          NODE_MODULES_SIZE=$(du -sh node_modules 2>/dev/null | cut -f1 || echo "Unknown")
        else
          PACKAGE_COUNT="N/A"
          NODE_MODULES_SIZE="N/A"
        fi
        
        echo "üìä Installation Metrics:"
        echo "  - Duration: ${INSTALL_DURATION}s"
        echo "  - Package Count: ${PACKAGE_COUNT}"
        echo "  - node_modules Size: ${NODE_MODULES_SIZE}"
        
        echo "PACKAGE_COUNT=${PACKAGE_COUNT}" >> $GITHUB_ENV
        echo "NODE_MODULES_SIZE=${NODE_MODULES_SIZE}" >> $GITHUB_ENV
    
    - name: CPU Performance Benchmark
      run: |
        set -e
        echo "üî• Running CPU-intensive benchmarks..."
        CPU_START=$(date +%s)
        
        # Run CPU benchmarks multiple times
        for i in $(seq 1 $TEST_ITERATIONS); do
          echo "üîÑ CPU Benchmark Iteration $i/$TEST_ITERATIONS"
          yarn test --testNamePattern="Performance Benchmarks" --verbose --testTimeout=60000
        done
        
        CPU_END=$(date +%s)
        CPU_DURATION=$((CPU_END - CPU_START))
        if ! [[ "$CPU_DURATION" =~ ^[0-9]+$ ]]; then
          echo "Error: CPU_DURATION is not a number!" >&2
          exit 1
        fi
        echo "CPU_DURATION=${CPU_DURATION}" >> $GITHUB_ENV
        
        echo "‚ö° CPU benchmarks completed in ${CPU_DURATION}s"
    
    - name: Memory and I/O Benchmark
      run: |
        set -e
        echo "üíæ Running memory and I/O benchmarks..."
        MEMORY_START=$(date +%s)
        
        # Run memory-intensive tests
        yarn test --testNamePattern="memory|Memory" --verbose --testTimeout=60000
        
        # File I/O benchmark (Linux only)
        if [ "${{ runner.os }}" = "Linux" ]; then
          echo "üìÅ File I/O Performance Test"
          dd if=/dev/zero of=test_file_1mb bs=1M count=1 2>/dev/null
          dd if=/dev/zero of=test_file_10mb bs=1M count=10 2>/dev/null
          dd if=/dev/zero of=test_file_100mb bs=1M count=100 2>/dev/null
          time cp test_file_100mb test_file_copy
          time gzip test_file_copy
          time gunzip test_file_copy.gz
          rm -f test_file_* test_file_copy*
        else
          echo "File I/O benchmarks are only run on Linux runners."
        fi
        
        MEMORY_END=$(date +%s)
        MEMORY_DURATION=$((MEMORY_END - MEMORY_START))
        if ! [[ "$MEMORY_DURATION" =~ ^[0-9]+$ ]]; then
          echo "Error: MEMORY_DURATION is not a number!" >&2
          exit 1
        fi
        echo "MEMORY_DURATION=${MEMORY_DURATION}" >> $GITHUB_ENV
        
        echo "üß† Memory and I/O benchmarks completed in ${MEMORY_DURATION}s"
    
    - name: Build Performance Test
      run: |
        set -e
        echo "üèóÔ∏è Testing build performance..."
        BUILD_START=$(date +%s)
        
        # TypeScript compilation
        echo "üìù TypeScript compilation test"
        time npx tsc --noEmit
        
        # Webpack build
        echo "üì¶ Webpack build test"
        time yarn build
        
        # Linting performance
        echo "üîç Linting performance test"
        time yarn lint
        
        BUILD_END=$(date +%s)
        BUILD_DURATION=$((BUILD_END - BUILD_START))
        if ! [[ "$BUILD_DURATION" =~ ^[0-9]+$ ]]; then
          echo "Error: BUILD_DURATION is not a number!" >&2
          exit 1
        fi
        echo "BUILD_DURATION=${BUILD_DURATION}" >> $GITHUB_ENV
        
        echo "üî® Build operations completed in ${BUILD_DURATION}s"
    
    - name: Network Performance Test
      run: |
        set -e
        echo "üåê Testing network performance..."
        NETWORK_START=$(date +%s)
        
        # Download speed test (Linux only)
        if [ "${{ runner.os }}" = "Linux" ]; then
          echo "‚¨áÔ∏è Download speed test"
          time curl -L -o /dev/null -s https://github.com/microsoft/TypeScript/archive/refs/heads/main.zip
          echo "üîÑ Concurrent download test"
          time (
            curl -L -o /dev/null -s https://registry.npmjs.org/react/latest &
            curl -L -o /dev/null -s https://registry.npmjs.org/vue/latest &
            curl -L -o /dev/null -s https://registry.npmjs.org/angular/latest &
            curl -L -o /dev/null -s https://registry.npmjs.org/express/latest &
            curl -L -o /dev/null -s https://registry.npmjs.org/webpack/latest &
            wait
          )
        else
          echo "Network benchmarks are only run on Linux runners."
        fi
        
        NETWORK_END=$(date +%s)
        NETWORK_DURATION=$((NETWORK_END - NETWORK_START))
        if ! [[ "$NETWORK_DURATION" =~ ^[0-9]+$ ]]; then
          echo "Error: NETWORK_DURATION is not a number!" >&2
          exit 1
        fi
        echo "NETWORK_DURATION=${NETWORK_DURATION}" >> $GITHUB_ENV
        
        echo "üì° Network tests completed in ${NETWORK_DURATION}s"
    
    - name: Calculate Total Performance
      run: |
        set -e
        WORKFLOW_END_TIME=$(date +%s)
        TOTAL_DURATION=$((WORKFLOW_END_TIME - WORKFLOW_START_TIME))
        if ! [[ "$TOTAL_DURATION" =~ ^[0-9]+$ ]]; then
          echo "Error: TOTAL_DURATION is not a number!" >&2
          exit 1
        fi
        
        # Calculate performance score (baseline: ubuntu-latest = 100)
        BASELINE_TIME=300  # Expected time for ubuntu-latest
        PERFORMANCE_SCORE=$((BASELINE_TIME * 100 / TOTAL_DURATION))
        echo "PERFORMANCE_SCORE=${PERFORMANCE_SCORE}" >> $GITHUB_ENV
        
        echo "## üìä Runner Performance Summary" >> $GITHUB_STEP_SUMMARY
        echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| **Runner Type** | ${{ env.RUNNER_TYPE }} |" >> $GITHUB_STEP_SUMMARY
        echo "| **Node.js Version** | ${{ matrix.node-version }} |" >> $GITHUB_STEP_SUMMARY
        echo "| **Dependencies Install** | ${INSTALL_DURATION}s |" >> $GITHUB_STEP_SUMMARY
        echo "| **CPU Benchmarks** | ${CPU_DURATION}s |" >> $GITHUB_STEP_SUMMARY
        echo "| **Memory & I/O** | ${MEMORY_DURATION}s |" >> $GITHUB_STEP_SUMMARY
        echo "| **Build Operations** | ${BUILD_DURATION}s |" >> $GITHUB_STEP_SUMMARY
        echo "| **Network Tests** | ${NETWORK_DURATION}s |" >> $GITHUB_STEP_SUMMARY
        echo "| **Total Duration** | **${TOTAL_DURATION}s** |" >> $GITHUB_STEP_SUMMARY
        echo "| **Performance Score** | **${PERFORMANCE_SCORE}** |" >> $GITHUB_STEP_SUMMARY
        
        echo "### üìà Performance Breakdown" >> $GITHUB_STEP_SUMMARY
        echo "- **Package Count**: ${PACKAGE_COUNT}" >> $GITHUB_STEP_SUMMARY
        echo "- **node_modules Size**: ${NODE_MODULES_SIZE}" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Iterations**: ${TEST_ITERATIONS}" >> $GITHUB_STEP_SUMMARY
        
        # Performance rating
        if [ $PERFORMANCE_SCORE -gt 150 ]; then
          echo "- **Rating**: üöÄ Excellent" >> $GITHUB_STEP_SUMMARY
        elif [ $PERFORMANCE_SCORE -gt 120 ]; then
          echo "- **Rating**: ‚ö° Very Good" >> $GITHUB_STEP_SUMMARY
        elif [ $PERFORMANCE_SCORE -gt 100 ]; then
          echo "- **Rating**: ‚úÖ Good" >> $GITHUB_STEP_SUMMARY
        elif [ $PERFORMANCE_SCORE -gt 80 ]; then
          echo "- **Rating**: ‚ö†Ô∏è Average" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **Rating**: üêå Below Average" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Generate Performance Report
      run: |
        # TODO: Integrate notification or fail workflow if needed
        # Create detailed performance report
        cat > performance-report.json << EOF
        {
          "runner_type": "${{ env.RUNNER_TYPE }}",
          "node_version": "${{ matrix.node-version }}",
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "metrics": {
            "install_duration": ${INSTALL_DURATION},
            "cpu_duration": ${CPU_DURATION},
            "memory_duration": ${MEMORY_DURATION},
            "build_duration": ${BUILD_DURATION},
            "network_duration": ${NETWORK_DURATION},
            "total_duration": ${TOTAL_DURATION},
            "performance_score": ${PERFORMANCE_SCORE}
          },
          "environment": {
            "os": "${{ runner.os }}",
            "arch": "${{ runner.arch }}",
            "package_count": ${PACKAGE_COUNT},
            "node_modules_size": "${NODE_MODULES_SIZE}",
            "test_iterations": ${TEST_ITERATIONS}
          }
        }
        EOF
        
        echo "üìÑ Performance report generated"
        cat performance-report.json
    
    - name: Upload Performance Report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report-${{ env.RUNNER_TYPE }}-node${{ matrix.node-version }}
        path: |
          performance-report.json
          coverage/
        retention-days: 30
    
    - name: Performance Threshold Check
      run: |
        echo "üéØ Checking performance thresholds..."
        
        # Define thresholds based on runner type
        case "${{ env.RUNNER_TYPE }}" in
          "ubuntu-latest")
            MAX_INSTALL=120
            MAX_TOTAL=300
            ;;
          "ubuntu-latest-4-cores")
            MAX_INSTALL=80
            MAX_TOTAL=200
            ;;
          "ubuntu-latest-8-cores")
            MAX_INSTALL=60
            MAX_TOTAL=150
            ;;
          "ubuntu-latest-16-cores")
            MAX_INSTALL=45
            MAX_TOTAL=120
            ;;
          "windows-latest")
            MAX_INSTALL=180
            MAX_TOTAL=400
            ;;
          "macos-latest")
            MAX_INSTALL=150
            MAX_TOTAL=350
            ;;
          *)
            MAX_INSTALL=120
            MAX_TOTAL=300
            ;;
        esac
        
        # Check thresholds
        THRESHOLD_PASSED=true
        
        if [ ${INSTALL_DURATION} -gt ${MAX_INSTALL} ]; then
          echo "‚ùå Installation time exceeded threshold: ${INSTALL_DURATION}s > ${MAX_INSTALL}s"
          THRESHOLD_PASSED=false
        fi
        
        if [ ${TOTAL_DURATION} -gt ${MAX_TOTAL} ]; then
          echo "‚ùå Total time exceeded threshold: ${TOTAL_DURATION}s > ${MAX_TOTAL}s"
          THRESHOLD_PASSED=false
        fi
        
        if [ "$THRESHOLD_PASSED" = true ]; then
          echo "‚úÖ All performance thresholds passed!"
          echo "üìä Performance within expected ranges for ${{ env.RUNNER_TYPE }}"
        else
          echo "‚ö†Ô∏è Performance thresholds exceeded - consider investigating"
          # Don't fail the workflow, just warn
        fi
    
    - name: Upload logs and reports
      uses: actions/upload-artifact@v4
      with:
        name: runner-comparison-logs
        path: |
          install.log
          perf.log
          build.log
          lint.log
        retention-days: 14